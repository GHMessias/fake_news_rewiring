import torch
import random
from torch_geometric.utils import to_networkit
from rewiring.rewiring import rewiring
import os
from gensim.models.doc2vec import Doc2Vec, TaggedDocument
from nltk.tokenize import word_tokenize
from sklearn.neighbors import NearestNeighbors
import networkx as nx
from torch_geometric.data import Data
from torch_geometric.utils import from_networkx
from models.inference_models.models import *
from torch_geometric.nn import GAE
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix
from utils.bibliotecas import *

# def organize_data(data, L, rate, positive_class, alpha, beta, gamma, name):
def organize_data(data, args):
    '''
    Function that organize the data object.

    Parameters:
    data (torch_geometric.data): torch_geometric.data object to organize
    L (int): parameter to apply the PSRB algorithm
    rate (float): rate of positive nodes to select at random
    positive_class (int): positive class to determine with object is positive. The remaining classes will be consedered as negatives
    name (str): name of the dataset
    alpha, beta, gamma (float): parameters of phi function
    seed_number (int): seed value, None by default

    Returns:
    Return the organized data object with the necessary positive and unlabeled elements.
    '''

    # setting data.y to be binary
    data.y = torch.tensor([1 if data.y[x] == args.positive_class else 0 for x in range(data.num_nodes)])

    # listing all the positive elements (this variable will not be used in the training phase of any algorithm)
    all_positives = [x for x in range(data.num_nodes) if data.y[x] == 1]

    # Positive and unlabeled elements
    data.P = torch.tensor(random.sample(all_positives , int(args.rate * len(all_positives))))
    data.U = torch.tensor([x for x in range(data.num_nodes) if x not in data.P])

    # List of graphs generated by the PSRB algorithm
    data.graph_list = rewiring(to_networkit(data.edge_index, directed=False), args.L, data.P, alpha=args.alpha_psrb, beta=args.beta_psrb, gamma=args.gamma_psrb)

    # Creating a tensor for reliable negatives and positive (hipothesis) elements
    data.infered_y = torch.tensor([-1] * data.num_nodes)
    for element in data.P:
        data.infered_y[element] = 1

    # Creating a test mask
    data.test_mask = torch.tensor([1 if x not in data.P else 0 for x in range(data.num_nodes)], dtype = torch.bool)

    # Naming the model
    data.name = args.name
    return data

# def doc2vec_matrix(path_true: str, path_fake: str, load = False, save_model = False, model_path = None):
def doc2vec_matrix(args):
    # Caminho da pasta onde estão os arquivos de texto

    # Lista de documentos processados
    documents_true = []
    documents_fake = []

    # Lê os arquivos .txt da pasta
    for filename in sorted(os.listdir(args.path_true)):
        if filename.endswith(".txt"):
            filepath = os.path.join(args.path_true, filename)
            with open(filepath, 'r', encoding='utf-8') as file:
                content = file.read().strip()
                # Tokeniza e cria o TaggedDocument para cada arquivo
                documents_true.append(TaggedDocument(words=word_tokenize(content.lower()), tags=[filename]))

    # Lê os arquivos .txt da pasta
    for filename in sorted(os.listdir(args.path_fake)):
        if filename.endswith(".txt"):
            filepath = os.path.join(args.path_fake, filename)
            with open(filepath, 'r', encoding='utf-8') as file:
                content = file.read().strip()
                # Tokeniza e cria o TaggedDocument para cada arquivo
                documents_fake.append(TaggedDocument(words=word_tokenize(content.lower()), tags=[filename]))
                    
    documents = documents_true + documents_fake
    labels = [1] * len(documents_true) + [0] * len(documents_fake)

    if args.load_model:
        # Carregando o modelo treinado (se necessário)
        model = Doc2Vec.load(args.model_path)
    else:
    # Treinando o modelo Doc2Vec
        print('Training D2V model')
        model = Doc2Vec(documents, vector_size=args.vector_size, window=args.window, alpha = args.alpha, min_alpha = args.min_alpha, epochs = args.epochs, dm_mean = args.dm_mean, dm = args.dm, dm_concat=args.dm_concat)

    if args.save_model:
        print('Saving D2V model')
        model.save(args.model_path)

    # Criando a matriz de vetores de cada documento
    matriz_vetores = [model.dv[doc.tags[0]] for doc in documents]

    return matriz_vetores, labels

def text_to_data(args):
    # Função para transformar os arquivos de texto tratados em um data object do pytorch. Essa função também aplica a transformação em grafo
    # Função para pegar os arquivos de data e transformar em um dataset pytorch
    
    doc2vec_rep = load_representation(args.representation_path)

    features = doc2vec_rep.text_vectors
    print(len(features))
    print(len(features[0]))

    x = torch.tensor(doc2vec_rep.text_vectors)
    y = np.load(args.representation_label_path)
    

    # Transformando o conjunto de dados em grafo
    G = create_knn_graph(x, args)

    edge_index = from_networkx(G).edge_index

    data = Data(x = x, y = y, edge_index = edge_index)
    print(data)
    return data


def create_knn_graph(vectors, args):
    # Cria um grafo vazio
    G = nx.Graph()
    
    # Adiciona os nós (um nó para cada vetor)
    for i, vec in enumerate(vectors):
        G.add_node(i, vector=vec)
    
    if args.graph_type == 'knn':
        # Usando NearestNeighbors para encontrar os k vizinhos mais próximos
        nbrs = NearestNeighbors(n_neighbors=args.k, algorithm='auto').fit(vectors)
        distances, indices = nbrs.kneighbors(vectors)
        
        # Adiciona arestas entre os k vizinhos mais próximos
        for i, neighbors in enumerate(indices):
            for j, neighbor_index in enumerate(neighbors[1:]):  # Ignora o primeiro vizinho (que é o próprio nó)
                G.add_edge(i, neighbor_index, weight=distances[i][j+1])
    
    return G

def get_model(model_name, data, args):
    '''
    Function to return the model given the model_name. To add new models, please create a new class at models.models file.

    Parameter:
    model_name (str): String that represents the model
    data (pytorch.data.data): data object to create the model
    kwargs: L (int): number of iterations that the data contains for the rewiring method
            hid_dim (int): number of hidden neurons
            out_dim (int): number of output neurons
            activation_function (builtin_function_or_method): activation function (default torch.relu)

    Returns:
    model
    '''
    if model_name == 'CCRNE':
        return CCRNE(data)
    if model_name == 'LP_PUL':
        # return LP_PUL(data, graph = to_networkx(data, to_undirected = True))
        return LP_PUL(data)
    if model_name == 'MCLS':
        return MCLS(data)
    if model_name == 'OCSVM':
        return OCSVM(data)
    if model_name == 'PU_LP':
        return PU_LP(data)
    if model_name == 'RCSVM':
        return RCSVM(data)
    if model_name == 'RGCN':
        return GAE(encoder = RGCN(data.x.shape[1], hidden_size = args.hid_dim, output_size=args.out_dim, L = args.L))
    if model_name == 'GCN':
        return GAE(encoder = GCN(data.x.shape[1], hidden_channels=args.hid_dim, out_channels=args.out_dim))

def evaluate(y_true, y_pred, pos_label = 1, verbose = True):
    acc =  round(accuracy_score(y_true, y_pred), 4)
    f1 = round(f1_score(y_true, y_pred, pos_label = pos_label),4)
    recall = round(recall_score(y_true, y_pred, pos_label = pos_label),4)
    precision = round(precision_score(y_true, y_pred, pos_label=pos_label),4)

    return {'acc':[acc], 'f1':[f1], 'recall':[recall], 'precision':[precision]}